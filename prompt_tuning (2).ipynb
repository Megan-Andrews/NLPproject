{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ace3176-946d-4b8f-921f-8234614f44e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "import argparse, os, string, sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import random \n",
    "\n",
    "from torch.optim import AdamW\n",
    "import argparse, os, string, sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, get_linear_schedule_with_warmup\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from peft import PromptEmbedding, PromptTuningConfig, PrefixTuningConfig\n",
    "from peft import PromptTuningConfig, PromptTuningInit, get_peft_model\n",
    "\n",
    "import json\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1d6a73db-bfd5-4bf6-906b-be6a20505ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvoDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_len, num_virtual_tokens=None):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.num_vitual_tokens = num_virtual_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def create_text(self, idx):\n",
    "        \"\"\"\n",
    "        Extracts a segment of the chat up to a randomly selected system message \n",
    "        occurring after the third message in the conversation, along with the target system message.\n",
    "        \n",
    "        Parameters:\n",
    "        - idx: Index of the conversation in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "        - processed_chat: A string containing the conversation up to the target system message.\n",
    "        - target_message: The selected target system message.\n",
    "        \"\"\"\n",
    "        \n",
    "        chat = []\n",
    "        sys_idx = []\n",
    "\n",
    "        # Attempt to load the JSON data safely\n",
    "        try:\n",
    "            dialog = json.loads(self.texts[idx]['text'])['dialog']\n",
    "        except (KeyError, ValueError) as e:\n",
    "            print(f\"Error loading conversation data: {e}\")\n",
    "            return \"\", \"\"\n",
    "\n",
    "\n",
    "        for e, i in enumerate(dialog):\n",
    "            if i['speaker'] == 'usr':\n",
    "                chat.append(i['text'])\n",
    "            if i['speaker'] == 'sys':\n",
    "                sys_idx.append(e)\n",
    "                chat.append(f\"[{i['strategy']}] \" + i['text'])\n",
    "            continue\n",
    "    \n",
    "        target_idx = int(random.choice(sys_idx))\n",
    "        while target_idx < 4:\n",
    "            target_idx = int(random.choice(sys_idx))\n",
    "\n",
    "        #target_idx = sys_idx[3]\n",
    "        processed_chat = \"\\n\".join([c for e, c in enumerate(chat[:target_idx]) if e not in sys_idx])\n",
    "\n",
    "        # print(processed_chat + \"\\n\\n\")\n",
    "        #print(chat[target_idx])\n",
    "\n",
    "        return  \"Provide suggestions, affirmations or reflection of feelings for the following person's needs. \"+processed_chat, chat[target_idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        processed_chat, target_message = self.create_text(idx)\n",
    "\n",
    "        # Tokenize the processed chat\n",
    "        inputs = self.tokenizer(\n",
    "            processed_chat,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        # Tokenize the target message\n",
    "        targets = self.tokenizer(\n",
    "            target_message,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        # Since the model expects labels to calculate loss, create labels by shifting the targets to the right\n",
    "        # This will be automatically handled if using a model that supports labels (like T5 or BART from Hugging Face)\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': targets['input_ids'].flatten(),\n",
    "        }\n",
    "\n",
    "        return inputs\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class CounselingDataset(Dataset):\n",
    "    def __init__(self, texts, responses, tokenizer, max_len, num_virtual_tokens=None):\n",
    "        self.texts = texts\n",
    "        self.responses = responses\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.num_virtual_tokens = num_virtual_tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        context = self.texts[idx]\n",
    "        response = self.responses[idx]\n",
    "        prompt = \"Provide counseling, suggestions and emotional support for this person:\"\n",
    "\n",
    "        # Tokenize the processed chat\n",
    "        inputs = self.tokenizer(\n",
    "            prompt + context,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            #padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        # Tokenize the target message\n",
    "        targets = self.tokenizer(\n",
    "            response,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            #padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        #print(f\"Input sequence length: {inputs['input_ids'].shape[1]}\")\n",
    "        #print(f\"Target sequence length: {targets['input_ids'].shape[1]}\")\n",
    "\n",
    "        # Calculate the number of padding zeros needed\n",
    "        num_padding_zeros = self.num_virtual_tokens\n",
    "\n",
    "        # Create a tensor of zeros for padding the attention mask\n",
    "        zeros_padding = torch.zeros(num_padding_zeros, dtype=torch.long).to(inputs['attention_mask'].device)\n",
    "\n",
    "        # Concatenate the zeros to the beginning of the attention mask tensor\n",
    "        #adjusted_attention_mask = torch.cat([zeros_padding, inputs['attention_mask'].flatten()], dim=0)\n",
    "        adjusted_inputs = torch.cat([zeros_padding, inputs['input_ids'].flatten()], dim=0)\n",
    "        return {\n",
    "            'input_ids': adjusted_inputs,\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': targets['input_ids'].flatten(),\n",
    "        }\n",
    "        \n",
    "def dynamic_padding_collate_fn(batch):\n",
    "    # Separate the components of the batch\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    \n",
    "    # Dynamically pad the sequences based on the longest sequence in the batch\n",
    "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=-100)  # Assuming -100 is ignored by your model's loss function\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_masks_padded,\n",
    "        'labels': labels_padded\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9636cb53-9ebf-4ca9-87a7-9fbe0805580c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 368,640 || all params: 60,875,264 || trainable%: 0.6055661623085528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100:   0%|                                                                                                           | 0/352 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "There should be 4 past states. 2 (past / key) for cross attention. Got 2 past key / value states",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 129\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5e-6\u001b[39m\n\u001b[1;32m    128\u001b[0m opts \u001b[38;5;241m=\u001b[39m Hyperparameters()        \n\u001b[0;32m--> 129\u001b[0m \u001b[43mread_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 84\u001b[0m, in \u001b[0;36mread_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     82\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 84\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpeft_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# print(dataset[\"Context\"][0])\u001b[39;00m\n\u001b[1;32m     86\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(dataset[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/NLPproject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NLPproject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/NLPproject/venv/lib/python3.10/site-packages/peft/peft_model.py:1164\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m peft_config\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m==\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mPREFIX_TUNING:\n\u001b[1;32m   1163\u001b[0m     past_key_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_prompt(batch_size)\n\u001b[0;32m-> 1164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/NLPproject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NLPproject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/NLPproject/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1748\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1745\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1748\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1749\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1751\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1758\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1759\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1760\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1761\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/NLPproject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NLPproject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/NLPproject/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1115\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1101\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1102\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         output_attentions,\n\u001b[1;32m   1113\u001b[0m     )\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/NLPproject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/NLPproject/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/NLPproject/venv/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:684\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    681\u001b[0m expected_num_past_key_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m4\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(past_key_value) \u001b[38;5;241m!=\u001b[39m expected_num_past_key_values:\n\u001b[0;32m--> 684\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    685\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere should be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_num_past_key_values\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m past states. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    686\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2 (past / key) for cross attention. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mif\u001b[39;00m\u001b[38;5;250m \u001b[39mexpected_num_past_key_values\u001b[38;5;250m \u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m4\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01melse\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    687\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(past_key_value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m past key / value states\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    688\u001b[0m     )\n\u001b[1;32m    690\u001b[0m self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    691\u001b[0m cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[\u001b[38;5;241m2\u001b[39m:]\n",
      "\u001b[0;31mValueError\u001b[0m: There should be 4 past states. 2 (past / key) for cross attention. Got 2 past key / value states"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "def read_data():\n",
    "    # Load pre-trained DistilBERT model and tokenizer\n",
    "    # tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', output_attentions=True)\n",
    "    # model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2, output_attentions=True)\n",
    "    prompt_tuning_init_text = \"Provide counseling, suggestions and emotional support for this person:\"#\"Provide suggestions, affirmations or reflection of feelings for the following person's needs. \"\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"google-t5/t5-small\",legacy=False)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n",
    "    #tokenizer = GPT2Tokenizer.from_pretrained(\"distilgpt2\")\n",
    "    #tokenizer.pad_token = tokenizer.eos_token\n",
    "    #model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
    "\n",
    "    prompt_config = PromptTuningConfig(\n",
    "    peft_type=\"PROMPT_TUNING\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    num_virtual_tokens=len(tokenizer(prompt_tuning_init_text)[\"input_ids\"]),\n",
    "    token_dim=768,\n",
    "    num_transformer_submodules=1,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    prompt_tuning_init=\"TEXT\",\n",
    "    prompt_tuning_init_text=prompt_tuning_init_text,\n",
    "    tokenizer_name_or_path=\"t5-small\",\n",
    "    )\n",
    "    \n",
    "    num_virtual_tokens = 20\n",
    "    prefix_config = PrefixTuningConfig(\n",
    "    peft_type=\"PROMPT_TUNING\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    num_virtual_tokens=num_virtual_tokens,\n",
    "    token_dim=768,\n",
    "    num_transformer_submodules=1,\n",
    "    num_attention_heads=12,\n",
    "    num_layers=12,\n",
    "    encoder_hidden_size=768,\n",
    "    )\n",
    "    \n",
    "    peft_model = get_peft_model(model, prefix_config)\n",
    "    peft_model.print_trainable_parameters()\n",
    "\n",
    "    # Define optimizer and learning rate\n",
    "    optimizer = AdamW(model.parameters(), lr=opts.lr)\n",
    "    \n",
    "    # Define training parameters\n",
    "    epochs = opts.epochs\n",
    "    batch_size = opts.batchsize\n",
    "\n",
    "    max_len = 512  # Maximum sequence length for DistilBERT\n",
    "\n",
    "    # Move model to appropriate device (GPU if available, else CPU)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    peft_model.to(device)\n",
    "\n",
    "    dataset = load_dataset(\"Amod/mental_health_counseling_conversations\")[\"train\"]\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(dataset['Context'], dataset['Response'], test_size=0.2, random_state=42)\n",
    "\n",
    "    train_dataset = CounselingDataset(train_texts, train_labels, tokenizer, max_len, num_virtual_tokens)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=dynamic_padding_collate_fn)\n",
    "    \n",
    "    val_dataset = CounselingDataset(val_texts, val_labels, tokenizer, max_len, num_virtual_tokens)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=dynamic_padding_collate_fn)\n",
    "\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_loader) * epochs),\n",
    "    )\n",
    "    # Iterate over epochs\n",
    "    for epoch in range(epochs):\n",
    "        peft_model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        # Wrap train_loader with tqdm\n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f'Epoch {epoch + 1}/{epochs}', unit='batch')):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = peft_model(input_ids, labels=labels,  attention_mask=attention_mask,)\n",
    "            # print(dataset[\"Context\"][0])\n",
    "            inputs = tokenizer(dataset[\"Context\"][0], return_tensors=\"pt\")\n",
    "            # print(inputs)\n",
    "           \n",
    "            # if batch_idx == 10:\n",
    "            #     print(model.base_model.prepare_inputs_for_generation)\n",
    "            #     output_token_ids = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device))\n",
    "            #     decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in output_token_ids]\n",
    "            #     for text in decoded_texts:\n",
    "            #         print(\"Context \\n\", text)\n",
    "            #     decoded_labels = [tokenizer.decode(ids, skip_special_tokens=True) for ids in labels]\n",
    "            #     for label in decoded_labels:\n",
    "            #         print(\"Predicted Response \\n\",  label)\n",
    "            #     print(\"Real Response \\n\", dataset[\"Response\"][0])\n",
    "            #     break\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        \"\"\"\n",
    "        inputs = tokenizer(prompt_tuning_init_text + dataset[\"Context\"][0], return_tensors=\"pt\")\n",
    "        #print(model.base_model.prepare_inputs_for_generation)\n",
    "        output_token_ids = model.generate(input_ids=inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device))\n",
    "        decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in output_token_ids]\n",
    "        print(\"Context \\n\", dataset[\"Context\"][0])\n",
    "        decoded_labels = [tokenizer.decode(ids, skip_special_tokens=True) for ids in labels]\n",
    "        for label in decoded_labels:\n",
    "            print(\"Predicted Response \\n\",  label)\n",
    "        print(\"Real Response \\n\", dataset[\"Response\"][0])\n",
    "        \"\"\"\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Training Loss: {avg_train_loss:.4f}')\n",
    "\n",
    "\n",
    "class Hyperparameters():\n",
    "    def __init__(self):\n",
    "        self.inputfile = os.path.join('data', 'input', 'ddr.csv')\n",
    "        self.epochs = 100\n",
    "        self.batchsize = 8\n",
    "        self.lr = 5e-6\n",
    "\n",
    "opts = Hyperparameters()        \n",
    "read_data()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71116e7-450a-4a79-8808-cafadb8cc3ca",
   "metadata": {},
   "source": [
    "-b 2\n",
    "Context \n",
    " I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\n",
    "   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\n",
    "   How can I change my feeling of being worthless to everyone?\n",
    "Predicted Response \n",
    " It sounds like you may be asking two different questions.With regard to what you said about your husband dressing as a female in your bedroom, I wonder if you would consider asking him more about this. If you choose to do that, I would suggest that you ask him whether a certain time is a good time to have a conversation and asking questions for five minutes or more that are related to you learning more about his experience. This can be difficult to do at times, particularly when you may want to offer your own opinions or become very anxious or of type. Consider thinking of a phrase that may help you to stay calm during the discussion. It may be helpful to think of yourself as asking questions as if you were an investigative reporter and using questions that start with words like \"what, how, who, where, when.\" Questions that start with \"why,\" can be very difficult to answer for some people and can be overwhelming because it often links to answers involving emotions that may or may not be understood. Also try restating what your husband is saying to make sure that you are understanding correctly. If what he is telling you is different than what you have heard or thought of for many years, it may be challenging to follow his meaning initially. Remember that listening to your husband does not imply agreement with what he is saying, just that you are following and looking to understand what he is experiencing. I also recommend sticking to one topic for the conversation, but this could be done with many different topics over time.You could also see if he would be willing to have a discussion where he listens like an investigative reporter to learn more about the experience that you are having.As far as what you mentioned about the sexual experience, maybe if you can discuss what it is that you don't like and/or understand what it is that he does like, you could see if there is some middle ground here. It depends on what you both prefer.These types of conversations can be difficult to have for some couples, at least initially. Having structured conversations, such as the ones I've described briefly above, can feel awkward initially, but the reason it can be helpful is because it can lead to further understanding in a way that decreases the chances of having an argument.Also consider seeing a therapist in your area who specializes in couples to discuss some of these ideas.\n",
    "Real Response \n",
    " If everyone thinks you're worthless, then maybe you need to find new people to hang out with.Seriously, the social context in which a person lives is a big influence in self-esteem.Otherwise, you can go round and round trying to understand why you're not worthless, then go back to the same crowd and be knocked down again.There are many inspirational messages you can find in social media.  Maybe read some of the ones which state that no person is worthless, and that everyone has a good purpose to their life.Also, since our culture is so saturated with the belief that if someone doesn't feel good about themselves that this is somehow terrible.Bad feelings are part of living.  They are the motivation to remove ourselves from situations and relationships which do us more harm than good.Bad feelings do feel terrible.   Your feeling of worthlessness may be good in the sense of motivating you to find out that you are much better than your feelings today.\n",
    "Epoch 1/1, Training Loss: 18.4784\n",
    "\n",
    "\n",
    "\n",
    "Context \n",
    "\n",
    "\n",
    " I'm going through some things with my feelings and myself. I barely sleep and I do nothing but think about how I'm worthless and how I shouldn't be here.\n",
    "   I've never tried or contemplated suicide. I've always wanted to fix my issues, but I never get around to it.\n",
    "   How can I change my feeling of being worthless to everyone?\n",
    "\n",
    "   \n",
    "Predicted Response \n",
    "\n",
    "\n",
    " Firstly, I would like to say how amazing that you have been able to live through breast cancer, sexual abuse as well as your depression and anxiety. You are stronger than you may believe at this moment for being able to continue to grow and live through these experiences. Take a moment to acknowledge your strength and know that this strength will be an asset during the therapeutic process that you will be able to tap into to find increased self esteem and more strength to address the concerns you are having. There are never too many concerns to address in therapy sessions. I hope you can find a great therapist who you can trust to listen and work with you to identify and address the most concerning issues first. Once you begin to do this you may find an alleviation of symptoms and feelings which could allow you to grow your capacity to manage difficult feelings and situations and address additional concerns. Before you even realize it the issues you are having will begin to feel more manageable.\n",
    "\n",
    " \n",
    "Real Response \n",
    "\n",
    "\n",
    " If everyone thinks you're worthless, then maybe you need to find new people to hang out with.Seriously, the social context in which a person lives is a big influence in self-esteem.Otherwise, you can go round and round trying to understand why you're not worthless, then go back to the same crowd and be knocked down again.There are many inspirational messages you can find in social media.  Maybe read some of the ones which state that no person is worthless, and that everyone has a good purpose to their life.Also, since our culture is so saturated with the belief that if someone doesn't feel good about themselves that this is somehow terrible.Bad feelings are part of living.  They are the motivation to remove ourselves from situations and relationships which do us more harm than good.Bad feelings do feel terrible.   Your feeling of worthlessness may be good in the sense of motivating you to find out that you are much better than your feelings today.\n",
    "Epoch 1/1, Training Loss: 18.5471"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08efd8b5-bca8-489d-b9ff-ac66c198d7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntokenizer = BartTokenizer.from_pretrained(\\'facebook/bart-large\\')\\nmodel = BartForConditionalGeneration.from_pretrained(\\'facebook/bart-large\\')\\nmodel.to(device)\\nprint(\"loaded model\")\\n\\nchat = []\\nsys_idx = []\\nfor e, i in enumerate(json.loads(dataset[\\'test\\'][0][\\'text\\'])[\\'dialog\\']):\\n    if i[\\'speaker\\'] == \\'usr\\':\\n        #chat.append(\\'[Target] \\' + i[\\'text\\'])  \\n        chat.append(i[\\'text\\'])\\n    if i[\\'speaker\\'] == \\'sys\\':\\n        #if e > 3:\\n        #    sys_idx.append(e)\\n        #chat.append(f\" [Context] [{i[\\'strategy\\']}] \" + i[\\'text\\'])\\n       continue\\n    \\n# target_idx = int(random.choice(sys_idx))\\n# processed_chat = \"\\n\".join(chat[:target_idx])\\n\\n# print(processed_chat + \"\\n\\n\")\\n#print(chat[target_idx])\\nprocessed_chat = \"\\n\".join(chat)\\n# Tokenize and generate summary\\ninputs = tokenizer(processed_chat, return_tensors=\"pt\", max_length=1024, truncation=True)\\ninputs.to(device)\\nsummary_ids = model.generate(inputs[\\'input_ids\\'], max_length=200, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)\\nsummary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\\n\\nprint(summary)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "################## Developing Summarization ###################\n",
    "\n",
    "# from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "# dataset = load_dataset(\"Amod/mental_health_counseling_conversations\")\n",
    "# print(dataset)\n",
    "# Load BART\n",
    "\"\"\"\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large')\n",
    "model = BartForConditionalGeneration.from_pretrained('facebook/bart-large')\n",
    "model.to(device)\n",
    "print(\"loaded model\")\n",
    "\n",
    "chat = []\n",
    "sys_idx = []\n",
    "for e, i in enumerate(json.loads(dataset['test'][0]['text'])['dialog']):\n",
    "    if i['speaker'] == 'usr':\n",
    "        #chat.append('[Target] ' + i['text'])  \n",
    "        chat.append(i['text'])\n",
    "    if i['speaker'] == 'sys':\n",
    "        #if e > 3:\n",
    "        #    sys_idx.append(e)\n",
    "        #chat.append(f\" [Context] [{i['strategy']}] \" + i['text'])\n",
    "       continue\n",
    "    \n",
    "# target_idx = int(random.choice(sys_idx))\n",
    "# processed_chat = \"\\n\".join(chat[:target_idx])\n",
    "\n",
    "# print(processed_chat + \"\\n\\n\")\n",
    "#print(chat[target_idx])\n",
    "processed_chat = \"\\n\".join(chat)\n",
    "# Tokenize and generate summary\n",
    "inputs = tokenizer(processed_chat, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "inputs.to(device)\n",
    "summary_ids = model.generate(inputs['input_ids'], max_length=200, min_length=10, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(summary)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb44d14-b672-403f-94a9-05d979f30e79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
